<div align="center">

# âš¡ FlowLens AI
### AI-Powered Business Process Intelligence Platform
**AMD Slingshot Hackathon 2026**

[![License](https://img.shields.io/badge/License-FlowLens%20Custom-blue.svg)](./LICENSE)
[![Python](https://img.shields.io/badge/Python-3.10%2B-3776AB?logo=python&logoColor=white)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.111+-009688?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![React](https://img.shields.io/badge/React-18+-61DAFB?logo=react&logoColor=black)](https://reactjs.org)
[![AMD ROCm](https://img.shields.io/badge/AMD-ROCm%206.0-ED1C24?logo=amd&logoColor=white)](https://rocm.docs.amd.com)
[![Ollama](https://img.shields.io/badge/Ollama-LLaMA%203.2-black?logo=ollama)](https://ollama.ai)

*Transforming raw WhatsApp-style workflow chat logs into actionable process intelligence â€” locally, privately, and AMD GPU-accelerated.*

</div>

---

## ğŸ¯ What is FlowLens AI?

Most businesses run critical workflows â€” invoice approvals, payment processing, refunds â€” through informal chat channels. The data is there, but it's invisible. **FlowLens AI turns that chaos into clarity.**

Upload a plain-text workflow chat log. FlowLens instantly parses every event, computes cycle times, detects SLA breaches, identifies bottlenecks, calculates financial impact, and deploys dual AI engines â€” **Google Gemini 2.5 Flash** for cloud inference and **LLaMA 3.2:3B via Ollama** for fully local, privacy-first, AMD GPU-accelerated inference â€” to deliver boardroom-ready insights in seconds.

**No database. No complex setup. No data ever leaving your machine if you choose local mode.**

---

## ğŸ† AMD Slingshot Relevance

FlowLens AI is purpose-built to leverage AMD hardware at every layer:

| AMD Technology | How FlowLens Uses It |
|---|---|
| **AMD GPU (RX 6000/7000, Instinct MI)** | Runs LLaMA 3.2 via Ollama + ROCm for fully local LLM inference |
| **PyTorch + ROCm 6.0** | GPU-accelerated Pearson correlation matrix computation across workflow stages |
| **Ryzen AI NPU (XDNA)** | Detected and reported in the live hardware dashboard |
| **ROCm Backend** | Real-time tokens/sec benchmark with CPU baseline comparison and speedup factor |

The application detects AMD hardware at startup, reports GPU name, compute units, VRAM, and ROCm version live in the UI, and shows a real-time inference performance panel (tokens/sec, GPU speedup vs CPU baseline, total inference time) with every analysis. On non-AMD hardware it gracefully falls back to CPU â€” **nothing breaks**.

---

## âœ¨ Core Features

### ğŸ“Š Process Mining Engine
- Parses WhatsApp-style, bracket-format, and multiple regional date format chat logs automatically
- Extracts invoice IDs, amounts (â‚¹), actors, timestamps, and action types from natural language
- Computes per-case cycle times, stage-level average durations, and standard deviations
- Identifies the bottleneck stage using comparative stage duration analysis
- Tracks SLA compliance per stage (Approval â‰¤ 2h, Payment â‰¤ 4h, Refund â‰¤ 6h) with breach counts

### ğŸ¤– Dual AI Inference Modes
| Mode | Engine | Privacy | Speed |
|---|---|---|---|
| â˜ï¸ **Cloud** | Gemini 2.5 Flash | Data sent to Google | Fastest |
| âš¡ **Local AMD** | LLaMA 3.2:3B via Ollama | 100% on-device | GPU-dependent |
| ğŸ“¡ **Local Stream** | LLaMA 3.2:3B (SSE) | 100% on-device | Real-time token stream |

Both modes generate identical structured insights: operational risks, bottleneck analysis, SLA improvement suggestions, and staffing recommendations.

### ğŸ’° Financial Impact Analysis
- Calculates monthly labor cost (based on stage hours Ã— hourly rate)
- Quantifies SLA breach penalties per breach event
- Computes cash-flow opportunity cost using Working Capital theory (WIP Ã— daily capital rate)
- Displays total value processed, average/median/min/max invoice values, and cost per case

### ğŸ”® What-If Simulation Engine
Four simulation scenarios powered by formal mathematical models:

| Scenario | Model Used |
|---|---|
| **Add Approvers** | M/M/c Queuing Theory with diminishing returns exponent |
| **Auto-Approval** | Linear invoice distribution model capped at 70% auto-approval |
| **Smart Routing** | Little's Law bottleneck optimisation (25% improvement factor) |
| **Custom Target** | Direct percentage reduction with proportional cost projection |

Each simulation returns: new cycle time, new SLA breach count, gross savings, net savings, annual projection, and payback period in months.

### ğŸ“„ AI-Generated SOP Documents
- Scaffold-first architecture: all structural data (SLAs, actors, KPIs, stage metrics) is computed deterministically in Python â€” the AI only writes professional prose
- Generates complete Standard Operating Procedures with title, objective, scope, prerequisites, role responsibilities, per-stage action descriptions, decision points, escalation paths, and exception handling
- Available via both Gemini (structured JSON schema output) and local LLaMA
- Includes version history, document ID, review date, and KPI targets auto-populated from live metrics

### ğŸ’¬ AI Copilot Chat
- Conversational process intelligence interface with full chat history
- Context-aware: every message is grounded in the live uploaded process data
- Available in both Gemini (cloud) and LLaMA (local) modes
- Answers questions like *"Which actor is causing the most SLA breaches?"* or *"What would happen if we added two approvers?"*

### ğŸ“‹ Implementation Plans & PDF Export
- One-click 8-week phased implementation plans for each optimisation scenario
- Role-assigned tasks with owner accountability per phase
- Export full simulation analysis reports as branded PDF (via jsPDF) with financial tables, recommendations, and process context

### âš¡ AMD Hardware Dashboard
- Live detection of AMD GPU model, compute units, VRAM, and ROCm version via `rocminfo`
- Ryzen AI NPU (XDNA) detection
- Real-time inference benchmark: tokens/sec, GPU speedup vs 8 TPS CPU baseline, total inference time
- GPU-accelerated Pearson correlation matrix between workflow stages (computed via PyTorch tensors on AMD GPU)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FlowLens AI                             â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   React 18   â”‚ â†HTTPâ†’  â”‚    FastAPI Backend           â”‚  â”‚
â”‚  â”‚   Frontend   â”‚         â”‚                              â”‚  â”‚
â”‚  â”‚              â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â€¢ Dashboard â”‚         â”‚  â”‚  Process Mining Engine  â”‚ â”‚  â”‚
â”‚  â”‚  â€¢ What-If   â”‚         â”‚  â”‚  (Pure Python)          â”‚ â”‚  â”‚
â”‚  â”‚  â€¢ SOP Gen   â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚  â€¢ Copilot   â”‚         â”‚                              â”‚  â”‚
â”‚  â”‚  â€¢ PDF Exportâ”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚ Gemini  â”‚  â”‚  Ollama  â”‚  â”‚  â”‚
â”‚                           â”‚  â”‚  2.5F   â”‚  â”‚ LLaMA3.2 â”‚  â”‚  â”‚
â”‚                           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚                           â”‚                    â†‘          â”‚  â”‚
â”‚                           â”‚             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚                           â”‚             â”‚  AMD GPU     â”‚  â”‚  â”‚
â”‚                           â”‚             â”‚  ROCm 6.0    â”‚  â”‚  â”‚
â”‚                           â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Backend:** Python 3.10+ Â· FastAPI Â· Uvicorn Â· httpx Â· PyTorch (ROCm) Â· python-dotenv  
**Frontend:** React 18 Â· Recharts Â· react-router-dom Â· jsPDF  
**AI:** Google Gemini 2.5 Flash (cloud) Â· LLaMA 3.2:3B via Ollama (local)  
**AMD Stack:** ROCm 6.0 Â· PyTorch ROCm build Â· rocminfo

---

## ğŸš€ Getting Started

### Prerequisites

Install these manually before anything else:

| Requirement | Download | Notes |
|---|---|---|
| **Python 3.10+** | [python.org](https://www.python.org/downloads/) | Required for backend |
| **Node.js 18+** | [nodejs.org](https://nodejs.org) | Required for frontend |
| **Ollama** | [ollama.ai/download](https://ollama.ai/download) | Required for local AI mode |

> **AMD GPU users:** For GPU-accelerated inference, additionally install [ROCm 6.0](https://rocm.docs.amd.com/) and the ROCm PyTorch build (see Step 4).

---

### Installation

**Step 1 â€” Clone the repository**
```bash
git clone https://github.com/YOUR_USERNAME/flowlens-ai.git
cd flowlens-ai
```

**Step 2 â€” Set up your environment variables**
```bash
cp .env.example .env
```
Open `.env` and add your Gemini API key (free at [aistudio.google.com](https://aistudio.google.com/app/apikey)):
```
GEMINI_API_KEY=your_gemini_api_key_here
```
> Local AMD mode works without a Gemini key â€” it's only needed for cloud inference.

**Step 3 â€” Install Python dependencies**
```bash
pip install -r requirements.txt
```

**Step 4 â€” (Optional) AMD GPU acceleration**

For AMD GPU with ROCm 6.0:
```bash
pip install torch --index-url https://download.pytorch.org/whl/rocm6.0
```
For CPU-only PyTorch:
```bash
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

**Step 5 â€” Launch**
```bash
python start.py
```

That's it. `start.py` automatically handles:
- âœ… Running `npm install` if `node_modules` is missing or outdated
- âœ… Starting the Ollama server if not already running
- âœ… Pulling `llama3.2:3b` if not already downloaded (~2GB, first run only)
- âœ… Starting the FastAPI backend on `http://localhost:8000`
- âœ… Starting the React frontend on `http://localhost:3000`
- âœ… Opening your browser automatically

---

### Services After Launch

| Service | URL |
|---|---|
| **App** | http://localhost:3000 |
| **Backend API** | http://localhost:8000 |
| **API Docs (Swagger)** | http://localhost:8000/docs |

Press `Ctrl+C` in the terminal to stop all services.

---

## ğŸ§ª Demo Logs

Three ready-to-use demo chat logs are included in `/demo_logs/` to immediately showcase the full range of FlowLens AI's capabilities:

| File | Scenario | Cases | Actors | Efficiency | Highlight |
|---|---|---|---|---|---|
| `demo_log_1_critical.txt` | ğŸ”´ Critical Process | 50 | 6 | Very Low | 100% SLA breach rate, all invoices breached â€” maximum AI risk alerts and savings potential |
| `demo_log_2_moderate.txt` | ğŸŸ¡ Moderate Process | 70 | 6 | Medium | Heavy PAYMENT bottleneck, high actor variance â€” showcases bottleneck detection |
| `demo_log_3_efficient.txt` | ğŸŸ¢ Efficient Process | 70 | 6 | High (â‰ˆ94) | Only 1 SLA breach, fast cycle times â€” great contrast benchmark |

**Upload any of these on the dashboard to instantly see the full analysis pipeline in action.**

---

## ğŸ“ Project Structure

```
flowlens-ai/
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py              # FastAPI app â€” process mining, AI inference, simulation
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js           # Full React application (single-file architecture)
â”‚   â”‚   â””â”€â”€ App.css          # Design system â€” dark theme, typography, components
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ demo_logs/
â”‚   â”œâ”€â”€ demo_log_1_critical.txt
â”‚   â”œâ”€â”€ demo_log_2_moderate.txt
â”‚   â””â”€â”€ demo_log_3_efficient.txt
â”‚
â”œâ”€â”€ start.py                 # One-click launcher â€” boots all services automatically
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env.example             # Environment variable template
â”œâ”€â”€ .gitignore
â””â”€â”€ LICENSE
```

---

## ğŸ“‹ Log Format

FlowLens AI parses chat logs in the following formats:

```
DD/MM/YYYY, HH:MM - ActorName: message text #invoiceNumber â‚¹amount
```

**Supported action keywords** (case-insensitive):

| Keyword in message | Maps to stage |
|---|---|
| `sent invoice` | INVOICE_SENT |
| `approved` | APPROVAL |
| `payment received` | PAYMENT |
| `refund initiated` | REFUND_INITIATED |
| `refund completed` | REFUND_COMPLETED |

**Example:**
```
03/01/2025, 08:05 - Priya: Sent invoice #1042 for â‚¹1,20,000 to Apex Corp
03/01/2025, 09:55 - Arjun: Approved invoice #1042 for â‚¹1,20,000
03/01/2025, 13:10 - Meena: Payment received for invoice #1042 for â‚¹1,20,000
```

Multiple date formats, 12h/24h time, and bracket-style WhatsApp exports are all supported.

---

## ğŸ”Œ API Reference

| Method | Endpoint | Description |
|---|---|---|
| `GET` | `/health` | Full system health check (Gemini, Ollama, AMD GPU) |
| `GET` | `/health/ollama` | Ollama + model availability check |
| `GET` | `/hardware/amd` | AMD GPU/NPU specs via rocminfo |
| `POST` | `/analyze` | Cloud analysis via Gemini 2.5 Flash |
| `POST` | `/analyze-local` | Local analysis via LLaMA 3.2 (blocking) |
| `POST` | `/analyze-stream` | Local analysis via LLaMA 3.2 (SSE streaming) |
| `POST` | `/simulate` | What-if scenario simulation |
| `POST` | `/chat` | Local copilot chat (LLaMA) |
| `POST` | `/chat-gemini` | Cloud copilot chat (Gemini) |
| `POST` | `/sop` | SOP generation via LLaMA |
| `POST` | `/sop-gemini` | SOP generation via Gemini (structured schema) |

Full interactive documentation available at `http://localhost:8000/docs` when the server is running.

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|---|---|
| **Backend Framework** | FastAPI 0.111+ with async/await throughout |
| **AI â€” Cloud** | Google Gemini 2.5 Flash (structured JSON schema output) |
| **AI â€” Local** | LLaMA 3.2:3B via Ollama (blocking + SSE streaming) |
| **AMD Acceleration** | PyTorch ROCm 6.0, rocminfo hardware detection |
| **Async HTTP** | httpx (Gemini API + Ollama communication) |
| **Frontend** | React 18, react-router-dom, Recharts |
| **PDF Export** | jsPDF (client-side, no server needed) |
| **Process Models** | M/M/c Queuing Theory, Little's Law, Working Capital theory |
| **Launcher** | Python subprocess orchestration (cross-platform) |

---

## ğŸ‘¥ Team

**FlowLens AI** â€” built for AMD Slingshot Hackathon 2026

| Name | Role |
|---|---|
| **Juluru Raghava Pranav** | Co-owner & Developer |
| **Kaarthikeyan Ganesh** | Co-owner & Developer |
| **Ashish Sheelam** | Co-owner & Developer |

---

## ğŸ“„ License

Copyright Â© 2026 FlowLens AI â€” Juluru Raghava Pranav, Kaarthikeyan Ganesh, Ashish Sheelam.

This project is protected under a custom license. Direct copying or misrepresentation of this work is strictly prohibited. Derivative works must credit the original authors. See [LICENSE](./LICENSE) for full terms.

---

<div align="center">
  <sub>Built with âš¡ for AMD Slingshot 2026 Â· FlowLens AI Â· All rights reserved</sub>
</div>
